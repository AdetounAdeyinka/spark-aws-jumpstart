{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Introduction\n",
    "![](spark-logo-trademark.png)\n",
    ">Apache Sparkâ„¢ is a fast and general engine for large-scale data processing.\n",
    "\n",
    "    Spark - Apache Spark has an advanced DAG execution engine that supports acyclic data flow and in-memory computing.\n",
    "Spark is written in Scala (and that is still likely the best place to use it) and runs on the JVM, but has a rich API in Python, R and JAVA. We will be using the Python API for this module, but will likely touch on the others in the future.\n",
    "\n",
    "Spark combines a rich and powerful dataframes API along with leveraging built-in sql, streaming, machine learning and graph computation capabilities. We can even combine these in the same analysis. \n",
    "\n",
    "We will be using it on top of hadoop and the yarn resource manager.\n",
    "\n",
    "We are going to follow the training from the creaters of Spark.\n",
    "## ___Resources___\n",
    "[Spark Documentation](http://spark.apache.org/docs/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session\n",
    "The spark session is essentially the entrypoint for interacting with spark. It is how we are connected to our cluster. It is stored in the `spark` variable. We use the `spark` variable to access the dataset, dataframe and sql APIs. We can also use it to access metadata about tables and databases that are cached.\n",
    "```python\n",
    "spark.catalog.listTables()\n",
    "```\n",
    "The spark session is also how we will read/write data.\n",
    "```python\n",
    "spark.read.csv('file.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x105721a90>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark # The spark session is store in this object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(100000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, we have a fully parallelized dataframe! From pandas, we might have expected the above to actually print out (or a subset of it to be accurate). So what's going on here? The answer lies in how Spark actually computes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations & Actions\n",
    ">Nature does not know extinction; all it knows is transformation.\n",
    "\n",
    "Spark is lazy. Or, I should say, _lazily_ evaluated. With Spark, there are exactly 2 types of operation, transformations and actions.\n",
    "### Transformations\n",
    "Transformations are operations that will not be excecuted when the cell or codeblock is run, but rather are accumulated, to be executed ___only___ when an action is initiated. This means that Spark is essentially building up a series of transformations (at least as long as no actions have been run) in a DAG. This allows Spark to optimize an entire transformation or series of transformations in the most efficient way and has the benefit of always being able to reconstruct datasets -- simply code the transformation up. \n",
    "\n",
    "#### Example DAG\n",
    "Here is an example DAG, taken from the web UI (port 4040).\n",
    "![](dag.png)\n",
    "### Actions\n",
    "Actions are what will actually give us a result back. Once a piece of code with an action is run, it is executed immediately and the associated transformations are now executed. \n",
    "\n",
    "This will all make sense once we get comfortable with the basic codebase, but you can think about it like this: transformations are blueprints (the DAG) to a particular dataset (which might be a subset or combination of other datasets) and actions are actually the building or exection of the blueprints.\n",
    "\n",
    "As an example, the `.describe()` method is a transformation -- the buleprints to a dataset. Calling `.show()` will actually execute the DAG and return the computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, id: string]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe() # Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|               id|\n",
      "+-------+-----------------+\n",
      "|  count|           100000|\n",
      "|   mean|          49999.5|\n",
      "| stddev|28867.65779668774|\n",
      "|    min|                0|\n",
      "|    max|            99999|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show() # Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "OK, let's see some of this in practice and start to get familiar with the API. We are going to use the diamonds dataset and perform some simple wrangling on it to try and understand these opersations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3 = \"s3://databricks-datasets-oregon/Rdatasets/data-001/csv/ggplot2/diamonds.csv\"\n",
    "df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\",\n",
    "\"true\").option(\"inferSchema\",\n",
    "\"true\").load(s3) # Use s3 when in aws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[carat: double, cut: string, color: string, clarity: string, depth: double, table: double, price: int, x: double, y: double, z: double]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used \"inferSchema\" above, but could also have entered it manually. The cost of inferring it is that we have to read through the dataset multiple times. Let's use a transformation, `.sample()` and then call an action after. We will see how the transformation isn't executed until we call the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = df.sample(True, .01, seed=0) # Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `.show()` action. You can think of this action as similar to pandas or R's `head()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|    cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "| 0.21|Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "| 0.23|   Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "| 0.29|Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
      "| 0.31|   Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can checkout the DAG physically at any time. Let's see what it has for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Sample 0.0, 0.01, true, 0\n",
      "+- InMemoryTableScan [carat#520, cut#521, color#522, clarity#523, depth#524, table#525, price#526, x#527, y#528, z#529]\n",
      "      +- InMemoryRelation [carat#520, cut#521, color#522, clarity#523, depth#524, table#525, price#526, x#527, y#528, z#529], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *FileScan csv [carat#49,cut#50,color#51,clarity#52,depth#53,table#54,price#55,x#56,y#57,z#58] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/acb/Documents/DataSci/sparkcourse/diamonds.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<carat:double,cut:string,color:string,clarity:string,depth:double,table:double,price:int,x:...\n"
     ]
    }
   ],
   "source": [
    "sample.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be a bit confusing at first, but there's the built in web UI that makes understanding it a whole lot easier (port 4040). Now let's make a slightly more complex example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = diamonds.groupBy(\"clarity\", \"color\").avg(\"price\") # Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+\n",
      "|clarity|color|        avg(price)|\n",
      "+-------+-----+------------------+\n",
      "|     IF|    F| 2750.836363636364|\n",
      "|    VS2|    G| 4416.256497656583|\n",
      "|   VVS2|    D|3351.1283905967452|\n",
      "|     IF|    D| 8307.369863013699|\n",
      "|    VS2|    D|  2587.22569239835|\n",
      "|    VS2|    H| 4722.414485696896|\n",
      "|    SI2|    D|3931.1014598540146|\n",
      "|    VS1|    E|2856.2943013270883|\n",
      "|    SI1|    H| 5032.414945054945|\n",
      "|   VVS1|    D|2947.9126984126983|\n",
      "|   VVS2|    F|3475.5128205128203|\n",
      "|     I1|    G| 3545.693333333333|\n",
      "|     I1|    F| 3342.181818181818|\n",
      "|    VS1|    F| 3796.717741935484|\n",
      "|    VS1|    J| 4884.461254612546|\n",
      "|    SI2|    H| 6099.895073576456|\n",
      "|   VVS1|    E|2219.8201219512193|\n",
      "|     I1|    H| 4453.413580246914|\n",
      "|   VVS1|    H|1845.6581196581196|\n",
      "|     I1|    D|3863.0238095238096|\n",
      "+-------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show() # Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df1.join(diamonds, on='clarity', how='inner').select(\"`avg(price)`\", \"carat\") # Don't worry about the actual join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [avg(price)#1038, carat#1102]\n",
      "+- *BroadcastHashJoin [clarity#3], [clarity#1105], Inner, BuildRight\n",
      "   :- *HashAggregate(keys=[clarity#3, color#2], functions=[avg(cast(price#6 as bigint))])\n",
      "   :  +- Exchange hashpartitioning(clarity#3, color#2, 200)\n",
      "   :     +- *HashAggregate(keys=[clarity#3, color#2], functions=[partial_avg(cast(price#6 as bigint))])\n",
      "   :        +- *Filter isnotnull(clarity#3)\n",
      "   :           +- InMemoryTableScan [color#2, clarity#3, price#6], [isnotnull(clarity#3)]\n",
      "   :                 +- InMemoryRelation [carat#0, cut#1, color#2, clarity#3, depth#4, table#5, price#6, x#7, y#8, z#9], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                       +- *FileScan csv [carat#49,cut#50,color#51,clarity#52,depth#53,table#54,price#55,x#56,y#57,z#58] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/acb/Documents/DataSci/sparkcourse/diamonds.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<carat:double,cut:string,color:string,clarity:string,depth:double,table:double,price:int,x:...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]))\n",
      "      +- *Filter isnotnull(clarity#1105)\n",
      "         +- InMemoryTableScan [carat#1102, clarity#1105], [isnotnull(clarity#1105)]\n",
      "               +- InMemoryRelation [carat#1102, cut#1103, color#1104, clarity#1105, depth#1106, table#1107, price#1108, x#1109, y#1110, z#1111], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *FileScan csv [carat#49,cut#50,color#51,clarity#52,depth#53,table#54,price#55,x#56,y#57,z#58] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/acb/Documents/DataSci/sparkcourse/diamonds.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<carat:double,cut:string,color:string,clarity:string,depth:double,table:double,price:int,x:...\n"
     ]
    }
   ],
   "source": [
    "df2.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, looks like we already have quite the DAG built up! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|       avg(price)|carat|\n",
      "+-----------------+-----+\n",
      "|2750.836363636364| 0.57|\n",
      "|2750.836363636364| 0.52|\n",
      "|2750.836363636364| 0.52|\n",
      "|2750.836363636364| 0.51|\n",
      "|2750.836363636364| 0.54|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377580"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "In data science, most of the work we do is interactive. We are constantly querying/wrangling our data and then once it's ready for analysis, we might create predictive models or perform some type of unsupervised learning to do something like anomaly detection. It can be frustrating to think we have to read data from disk constantly for each piece of this puzzle. Spark has a solution to this problem -- caching. Spark can store datasets in memory to significantly increase performance. Let's see an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 718 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[avg(price): double, carat: double]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377580"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count() # Load once more from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run on the cached dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 429 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This of course is still a small dataset, so imagine the performance increase in a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://192.168.2.7:4040'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
